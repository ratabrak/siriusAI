{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratabrak/siriusAI/blob/main/SiriusAI_dopWork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "rKNo33W5sW02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHaBNZ0Wqat5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import requests\n",
        "from PIL import Image\n",
        "from datasets import load_dataset\n",
        "from transformers import CLIPProcessor, CLIPModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = os.makedirs('data', exist_ok=True)\n",
        "model = CLIPModel.from_pretrained(\"Marqo/marqo-fashionCLIP\")\n",
        "processor = CLIPProcessor.from_pretrained(\"Marqo/marqo-fashionCLIP\")\n",
        "ds = load_dataset(\"ceyda/fashion-products-small\")\n",
        "for example in ds['train']:\n",
        "  with open(f\"data/{example['filename']}\", 'wb') as smt:\n",
        "    example['image'].save(smt, format='jpeg')"
      ],
      "metadata": {
        "id": "ThO-heSMs1U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def describe_clothing(image_path):\n",
        "    # Открываем изображение\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Пример списка текстовых описаний одежды\n",
        "    descriptions = [\n",
        "        \"A red dress with floral patterns\",\n",
        "        \"A casual blue denim jacket\",\n",
        "        \"A black leather jacket\",\n",
        "        \"A white cotton T-shirt\",\n",
        "        \"A pair of blue jeans\",\n",
        "        \"A formal black suit\",\n",
        "        \"A yellow summer dress\",\n",
        "    ]\n",
        "\n",
        "    # Применяем процессор для изображения и текста\n",
        "    inputs = processor(\n",
        "        text=descriptions,\n",
        "        images=image,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    # Прогоняем через модель\n",
        "    outputs = model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image  # Логиты для изображения\n",
        "    probs = logits_per_image.softmax(dim=1)  # Конвертируем в вероятности\n",
        "\n",
        "    # Находим наиболее вероятное описание\n",
        "    best_description_index = probs.argmax().item()\n",
        "    predicted_description = descriptions[best_description_index]\n",
        "\n",
        "    return f\"Описание одежды: {predicted_description}\"\n",
        "\n",
        "dirname = 'data2'\n",
        "for fname in os.listdir(dirname):\n",
        "  print(describe_clothing(dirname + '/' + fname))"
      ],
      "metadata": {
        "id": "Te3jkkz4yeL2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}