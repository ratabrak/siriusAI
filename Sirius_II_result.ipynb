{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratabrak/siriusAI/blob/main/Sirius_II_result.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ozxhkRRBX034",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec34876-f8ab-456e-c742-f8fbff7298c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Введите название файла: itman.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "\n",
        "!pip uninstall transformers -y;\n",
        "!pip install transformers==4.46.1;\n",
        "!pip install accelerate;\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n",
        "\n",
        "model_id = \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\"\n",
        "model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        ").to('cuda')\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "conversation = [\n",
        "    {\n",
        "\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "          {\"type\": \"text\", \"text\": \"Write a description of each individual piece of clothing item by item.\"},\n",
        "          {\"type\": \"image\"},\n",
        "        ],\n",
        "    },\n",
        "]\n",
        "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
        "\n",
        "the_name = input(\"Введите название файла: \")\n",
        "\n",
        "image_file = f\"/content/{the_name}\"\n",
        "raw_image = Image.open(image_file)\n",
        "inputs = processor(images=raw_image, text=prompt, return_tensors='pt').to('cuda', torch.float16)\n",
        "output = model.generate(**inputs, max_new_tokens=200, do_sample=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(processor.decode(output[0][2:], skip_special_tokens=True).split('\\n')[2])"
      ],
      "metadata": {
        "id": "U-9W955rbOc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0275c75f-c3bb-45e7-9f0f-7e2144e3a5a5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The man is wearing a black suit jacket, a white dress shirt, and a black tie. He also has a silver wristwatch on his left wrist.\n"
          ]
        }
      ]
    }
  ]
}